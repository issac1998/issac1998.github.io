As of this writing, the ingestion engine for
Facebook and Twitter keeps around 350TB of data in WAS
(before replication). In terms of transactions, the ingestion engine
has a peak traffic load of around 40,000 transactions per second
and does between two to three billion transactions per day (see
Section 7 for discussion of additional workload profiles).
  
problem remain
1.For this, WAS provides three properties that the CAP theorem [2] claims are difficult to achieve at the same time: strong consistency, high availability, and partition tolerance (see Section 8). 

  How?  

2.Since a major goal of WAS is to enable storage of massive
amounts of data, this global namespace must be able to address
exabytes of data and beyond. We discuss our global namespace
design in detail in Section 2. 
  
  How?

3.WAS provides cloud storage in the form of Blobs (user files),
Tables (structured storage), and Queues (message delivery).

  the form of 3 types is paralle or Progressive?

4.Multi-tenancy and Cost of Storage – To reduce storage cost,
many customers are served from the same shared storage
infrastructure. WAS combines the workloads of many different
customers with varying resource needs together so that
significantly less storage needs to be provisioned at any one point
in time than if those services were run on their own dedicated
hardware. 
  
     |
     FE Layers :Partition Maps,Cacheing objects,authnticate check
     |
    Partition Layer : high leverl data abstractions,index data,provide scale scalability both in object namespace and partition data in within a stamp(a object data maybe divided to diff partion layers) ,transcation/consistency managing,Cacheing object data.
     |
    Stream Layer :data store in disk and replication


Intra-Stamp Replication: sync,in Stream Layer replicate {Extents}, assure Stream data durable 
Inter-Stamp Replication: async,off the critical path,whole file duplication

Stream Layer

Append Only ,Only the last extent in the stream can be appended to. All of the prior extents in the stream are immutable. 

    Streams :A stream is an ordered list of pointers to extents which is maintained by the Stream Manager. 
    |
    Extents: FIle chunks.
    |
    Blocks:minimum unit,up to 4MB, When range read, read the whole Block cuz need to check blocks checksum
    |


The SM keeps track of the stream namespace, what extents are in each stream, and the extent
allocation across the Extent Nodes (EN). monitor all stream.Determain Extent nodes to be store

SM allocates Primary EN and other replicas for a extent, This state is now part of the stream’s metadata information held in the SM and cached on the client. 

Write Only write to Primary EN,Primary EN writes to other replicas,Primary EN does following things
 (a) determining the offset of the append in the extent, 
 (b) ordering appends when concurrent,
 (c) sending the append with its chosen offset to other replicas,
 (d) only returning success when all replicas succedd

When Extent is unsealed(still can be appended to ), the Primary EN never changed.



A client sends all write requests to the primary EN, but it can read from any replica, even for unsealed extents. 

weak consistency: (1)Primary acknowledged back to the client (2)Once an extent is sealed



Clients cached the extent,read or write on it ,no need to contact to SM until allocate next extent.

If write failed，SM will seal extent with commit length


Sealing:
seal at min commit length of all reachable EN,when  unreachable en becomes rechable,sync it with already sealed EN.

Read:Only read from a previous successful commit (identical in all replicas).

4.3.3 How to deal with network partition

4.5 read load rebalance 
read request comes along with  a deadline , EN determained whether this request can be fulfiiled within deadline,if not ,return err,request will go to another EN
When reads are issued for an extent that has three replicas, they
are submitted with a “deadline” value which specifies that the
read should not be attempted if it cannot be fulfilled within the
deadline

4.6 efficiency
write: (a) writes all of the data for the append to
the journal drive and (b) queues up the append to go to the data
disk where the extent file lives on that EN.  
 Once either succeeds,
success can be returned.If the journal succeeds first, the data is
also buffered in memory while it goes to the data disk, and any
reads for that data are served from memory until the data is on the
data disk. 

weird,cost a lot ,but truely effective.

5.Partition Layer
Figure 5: 
Stored in LSM
 commit log：all op applied
 metaData Stream: the MetaD
 Data Stream: snapshot 
store in Mem:
 Cache 
 Bloom Filter 

 Write-> write to commit log and in mem cache,periodically make check point.


lesson's learned 

load balancing without paramters

CAP fake P.... in the whole system can achieve 

paper:https://www.cs.purdue.edu/homes/csjgwang/CloudNativeDB/AzureStorageSOSP11.pdf

