---
title:  "CV"
search: true
categories:
  - Jekyll
  - Go
  - codes
  - src
last_modified_at: 2025-08-03T03:06:00-05:00
---
<table style="width:100%;">
  <tr>
    <td style="vertical-align:top; padding-right:20px;">

# 潘兆丰

<div>
  <span>
    <img src="assets/phone-solid.svg" width="18px"> 手机号码：15988788890（微信同号）/ 18868846460
  </span>
  ·
  <span>
    <img src="assets/envelope-solid.svg" width="18px"> 邮箱：issacpan98@gmail.com
  </span>
  ·
  
</div>

</td>
<td style="width:140px; vertical-align:top; text-align:right;">
  <!-- 把下面的路径替换成你的头像文件路径。建议 150x150 像素，圆形。 -->
  <img src="assets/avatar.jpg" alt="头像" style="width:140px; height:140px; object-fit:cover; border-radius:50%; border:2px solid #ddd;">
</td>

  </tr>
</table>

## 个人信息

* 男，1998年出生
* 求职意向：Go 开发工程师（业务方向）
* 工作经验：1年

## 教育经历

* 研究生，浙江大学，软件工程，2021.9–2024.6
* 本科，浙江工业大学，信息与计算科学，2017.9–2021.7

## 工作经历
**滴滴出行 实习生 基础平台 - 文件存储 2022.11–2024.05** 

**滴滴出行 后端开发工程师 基础平台 - 文件存储 2024.07–2025.10** 

部门主要产品为分布式文件存储系统，大致可以分为四个模块：存储元数据的 MetaData Server (MS) 模块，存储数据的 Blob Server (BS) 模块，存储集群信息及 topo 结构的 Root Server (RS) 以及执行后台任务调度的 Background (BG) 模块。

实习期间主要参与用户管理平台及元数据管理 MDS 模块的协助开发；正式工作期间主要负责 BG 和 BS 模块开发。
- 参与开发离线数据迁移，将百亿级别数据从多副本迁移至 EC 存储。
- 负责设计与开发存储引擎BS2.0的后台任务及BG调度框架，后台任务包含存储数据一致性检测/修复/GC等功能，BG负责调度上述任务。
- BS 2.0读写性能优化，包含 range-read、内存池优化等。

## 项目经历
**BS2.0 存储引擎 2024.10–2025.10**
  
背景：BS1.0 基于小集群设计，每个集群有单独的 Master 节点管理数据，运维成本高且磁盘利用率低。因此需要开发基于大集群的 BS2.0，并实现更加丰富的后台任务。

主要工作：
- 设计并开发后台任务：包含数据校验、数据修复及并发垃圾回收。
- 为以上后台任务开发调度框架，包含任务创建/查询/异常任务重启/限流等操作。
- 优化 BS2 读写性能。
           
困难：
- 任务调度框架的设计：可能存在数亿个数据副本，如何高效地查询、检测、调度？各模块挂掉后如何处理？
- 如何做限流，如何取得读写流量与后台流量的均衡，并尽可能少影响业务延迟？
- GC效率低，如何提升？
- 开发初期性能不如 BS1.0，如何优化？

解决 / 思考：

**离线 EC 迁移 2024.07–2024.10**

背景：早期业务使用多副本冗余存储，存在资源浪费、读取无法 stripe 化等问题，需要高效且可靠地将数据迁移至 EC（erasure coding，纠删码）存储。
  
主要工作：负责开发离线数据迁移，由 Master 扫描多副本数据，将待迁移数据写入 Migrate 中间表，Worker 从中间表消费任务并执行迁移。

困难：
- 百亿级别小文件迁移效率问题：Master/Worker 如何抢锁、任务如何分配、资源如何控制？
- Worker 消费速度跟不上，数据库中间表放不下，如何解决？
- 如何保证不漏迁？对于 TTL 卷等复杂场景如何适配？

解决：

**MDS 元存储引擎 2022.11–2024.05**

背景：随着元数据量增长，原有 MySQL 存储遇到性能瓶颈，需要基于 LSM Tree 的 KV 架构来支持海量数据存储。

主要工作：参与方案讨论并开发部分功能。

困难：
- Raft 相关：串行化优化如何与业务场景适配。

**Go 实现消息队列（尚未完成，实现约 90%） 2025.08–2025.10**

<https://github.com/issac1998/go-queue>
- 使用 Go 实现的强一致持久化消息队列，消息通过文件形式落盘，不同 Partition 作为多 Raft Group，持久化在多台机器上。
- 由 Controller 进行 Partition 分配和重平衡。
- 实现 exactly-once、Consumer Group、Dead Letter、延时队列、生产/消费事务，同时支持消息压缩、去重等功能。

**秒杀系统设计（尚未完成，实现约 50%） 2025.09–2025.10**

<https://github.com/issac1998/mall>

- 高并发秒杀系统架构设计，包含用户管理、库存管理、订单管理等模块，库存使用 Redis 做缓存。
- 使用消息队列削峰，并通过延时消息实现过期订单的检查。
- 使用 Redis 分片集群做库存缓存，将库存分散到不同分片上。