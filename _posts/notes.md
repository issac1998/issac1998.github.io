---
title:  "notes "
search: true
categories:
  - Jekyll
  - Go
  - codes
  - src
last_modified_at: 2025-10-10 T03:06:00-05:00
---

- [No-op](#no-op)
- [leader 先发送Append请求](#leader-先发送append请求)
- [No-op可以解决网络分区的问题吗](#no-op可以解决网络分区的问题吗)
- [DMA](#dma)
- [RDMA](#rdma)
- [DPDK](#dpdk)
- [io\_uring](#io_uring)
- [内存对齐](#内存对齐)
- [分布式锁](#分布式锁)
- [redis](#redis)
- [消息队列](#消息队列)
  - [](#)
  - [Leader负载均衡](#leader负载均衡)
  - [consumer阻塞？](#consumer阻塞)
  - [消息被消费后怎么处理？](#消息被消费后怎么处理)
  - [prodocuer exact-once](#prodocuer-exact-once)
  - [consumer exact-once](#consumer-exact-once)
  - [In sync](#in-sync)
  - [存在的问题](#存在的问题)
  - [kafka为什么不用raft存数据](#kafka为什么不用raft存数据)
  - [待解决](#待解决)
  - [用multi-raft 好处](#用multi-raft-好处)
  - [一致性](#一致性)
- [CV](#cv)
  - [Raft相关优化](#raft相关优化)


# No-op
No-op解决的是Raft迟迟不能Commit的问题，由于Raft只能commit自己任期的Entry（https://download.csdn.net/blog/column/10480536/134026228），当有老Leader还没来得及提交老Entry时，若没有新的请求来，老Entry迟迟不能提交，会影响线性一致读。

因此需要No-op提交当前term的Entry，并间接提交老Entry

# leader 先发送Append请求
传统流程先append log到磁盘，再发送Append请求给Follower
但其实可以并发发送Append请求给Follower，Follower将log写入磁盘后返回，防止部分盘写得慢(尤其是Leader)导致的问题

# No-op可以解决网络分区的问题吗
看想解决什么问题
1.完全网络分区，没有意义
2.原Leader和某台机器网络分区，其他连接都正常。No-op可以防止不断地重复交替选举。

# DMA
DMA:替代cpu的数据搬运功能，但也只能操作内核态
RDMA：write/read可直接操作用户内存，不需要CPU参与。send/recv需要cpu先分配一块buffer用于接受、发送，还是需要CPU参与。
RDMA可以实现bypass kernal:
1.数据无需cpu进行协议栈操作
2.用户态数据无需拷贝至内核态进行传输（传统连接方式需要cpu转换物理地址、用户态也没有权限调用硬件），也意味着需要实现虚拟地址的转换
用户程序

# RDMA
RDMA 网卡需要提前知道 “虚拟地址→物理地址” 的固定映射（通过MTT 表，即 Memory Translation Table），才能直接访问物理内存

write/read前需要先建立连接，连接可以是TCP也可以自定义，连接中注册memory region，pin 虚拟地址防止被swap，后续就可以直接操作用户内存了

Memeory region：在用户内存申请一块地址，并由memory region会在内存中创建一个映射表，将WQ的虚拟地址转换成真实地址，不需要CPU MMU和页表进行转换。同时pin这段内存防止被swap换出。通过memory key作权限控制，杜绝恶意用户对于本地或者远端内存的访问

Memory window：由于memory region修改权限需要陷入内核态，流程较长，因此memory window可以快速注册权限，并绑定到某人memory region 地址

Protection Domain:用于隔离Memory Region，将QP和memory region绑定分组，防止其他QP访问Memory region

CQ:WQ完成后会将执行结果放入CQ，用户程序可以通过poll(轮询)或注册中断函数来获取结果


用户态、内核态：除了下发WR（Post Send和Post Recv）和获取WC（Poll CQ和Request Completion Notification）这种用于数据交互的接口，以及Bind MW和AH的相关操作，其他所有操作都需要特权，即调用对应的Verbs API都需要陷入内核态。 陷入内核态是为了1.用户态是不安全的，有些资源不能暴露给用户态修改 2.需要建立虚拟地址到物理地址的静态映射

只有数据层面的bypass内核：RDMA的所谓内核Bypass，并不是整个流程都绕过了内核，而是在控制路径多次进入内核进行准备工作，万事俱备之后，才可以在数据路径上避免陷入内核时的开销。

Write：1.cpu将数据写入MR 2.驱动通知网卡doorbell 3.网卡发送取MR，解析，发送 4.对端网卡鉴权，写入内存，写入的内存地址是发送端指定的


Send: Send操作的数据流向和Write操作是相同的，差异在于Payload放在哪里是由Responder决定的， 所以不用指定写入的地址。对端网卡根据内容选择写在哪块内存中。


Q: 传统I/O模式为什么将数据从磁盘读取到内核空间缓冲区，然后再将数据从内核空间缓冲区拷贝到用户空间缓冲区了？为什么不直接将数据从磁盘读取到用户空间缓冲区就好？
A: 传统I/O模式之所以将数据从磁盘读取到内核空间缓冲区而不是直接读取到用户空间缓冲区，是为了减少磁盘I/O操作以此来提高性能。因为OS会根据局部性原理在一次read()系统调用的时候预读取更多的文件数据到内核空间缓冲区中，这样当下一次read()系统调用的时候发现要读取的数据已经存在于内核空间缓冲区中的时候只要直接拷贝数据到用户空间缓冲区中即可，无需再进行一次低效的磁盘I/O操作(注意：磁盘I/O操作的速度比直接访问内存慢了好几个数量级)。

但是如果使用MMAP，就不能用上内核缓冲区这块多拿的缓存了，直接写到page cache上。

总的来说，通过sendfile实现的零拷贝I/O只使用了2次用户空间与内核空间的上下文切换，以及3次数据的拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝（kernel buffer ——> socket buffer）。从Linux 2.4版本开始，操作系统底层提供了scatter/gather这种DMA的方式来从内核空间缓冲区中将数据直接读取到协议引擎中，而无需将内核空间缓冲区中的数据再拷贝一份到内核空间socket相关联的缓冲区中。

# DPDK
DPDK则是用mmap的方式映射了虚拟地址和物理内存，实现了userspace io，通用性相对更好，

简单来说，驱动负责准备 mbuf 并将其对应的物理地址填写到描述符中，然后告知网卡。网卡的 DMA 引擎读取描述符，根据其中的物理地址，直接将数据包写入或读出内存，最后更新描述符中的状态位。驱动通过轮询状态位来判断数据包是否就绪或发送完成。


采用大页，减少tlb miss
提供的用户态 PCIe 设备访问能力，将设备的寄存器mmap到用户态，绕过内核块层，直接控制设备（如 NVMe）。
使用轮询的方式处理网络事件，**通常会将轮询的线程绑定到一个核**，专门来轮询，避免了线程在核间迁移带来的缓存失效问题


DPDK

描述符 (Descriptor)： 一个存储在内存中的数据结构，可以被网卡（通过 DMA） 和 CPU（驱动） 共同访问和修改。它本质上是一个“工单”，包含了数据包存储的物理地址（DMA 地址） 和包的状态信息（如长度、校验和、状态位等）。网卡通过读取描述符知道该把数据包放在哪里，驱动通过检查描述符知道网卡的操作是否完成。
Mbuf (Memory Buffer)： DPDK 中管理数据包内存的核心结构。它存储了数据包的虚拟地址、元数据（如包长度、端口号、VLAN 信息等）以及指向实际数据包内容的指针。驱动操作的是 mbuf。


简单来说，驱动负责准备 mbuf 并将其对应的物理地址填写到描述符中，然后告知网卡。网卡的 DMA 引擎读取描述符，根据其中的物理地址，直接将数据包写入或读出内存，最后更新描述符中的状态位。驱动通过轮询状态位来判断数据包是否就绪或发送完成。

用户态驱动
mmap预先分配大量mbuf，DPDK 的 PMD（Poll Mode Driver）驱动在一个紧密的循环中轮询（Poll）rx_ring 中的描述符状态，检查 DD 位是否为 1，而不是依赖中断通知。这避免了中断开销，是高性能的关键。




# io_uring
通过CQ、SQ以及mmap共享内存，减少系统调用。

内核会从SQ中依次取出对应的io request实体，根据其定义的动作来执行对应的操作。由于用户只操作SQ尾部，而内核只操作头部，因此两者对于共享队列的访问并不会产生冲突，**节省了锁的开销**，同时也可以批量提交IO。

提供三种模式：中断模式(可通过系统调用 io_uring_enter() 提交IO请求，进入睡眠状态等待硬件完成。硬件在完成 I/O 后触发硬中断，中断处理程序唤醒等待的进程，通知 I/O 完成)，内核轮询模式 / 提交sqpoll轮询模式(创建内核线程执行SQ轮询。)

IO提交：找到一个空闲的 SQE，根据请求设置 SQE，并将这个 SQE 的索引放到 SQ 中。若是中断模式，还需要系统调用 io_uring_enter() 提交IO。若是内核轮询模式，可以立刻捕获到这次提交

IO执行完成：当 IO 完成时，内核负责将完成 IO 在 SQEs 中的 index 放到 CQ 中。在中断模式下，IO提交时等待中断返回完成CQ。若是轮询模式，用户态轮询CQ列表即可。


内核维护Open File Table，记录着文件描述(file description)，其中包含file offset和文件状态（O_sync,O_direct），以及文件系统的Inode信息。

每个进程tast_struct有fd列表，其值指向内核中的file description。如果进程1使用fork或dup或fcntl复制进程2的fd，则会对应至内核的同一个file description。



# 内存对齐
CPU访问内存时，由于底层内存存储模块读取规则是读取8个dram相同(i,j)位置的cell，这8个字节是严格按存储模块边界对齐的，因此CPU1次内存读区操作只能按8个字节边界对齐。因此需要内存对齐，减少内存读取。可见内存对齐的限制不是在CPU，而是在内存存储模块。事实上，很多CPU支持随机边界访问，但那只是CPU帮你读取两次内存罢了

在GO的结构体中，有许多优化与内存对齐相关。
1.改变结构体变量的顺序可以减少内存对齐的Padding量，尽量将相同大小的字段放一起
2.将高频访问的变量放在同一个内存8字节里，可以提高CPU 缓存命中率
3.加入padding字段，防止伪共享（类似C++的no_cache_line，但是go需要手动padding）
4.分离指针和非指针数据，可以优化GC快速跳过整块非指针区域

# 分布式锁
# redis 
SETNX不能指定expire，所以需要.set(lockKey, requestId, "NX", "PX", expireTime)
过期值要怎么设置，要不要有续约机制？
集群模式下，由于异步复制，可能导致两个客户端都持有锁。集群模式下，需要实现Redlock

# 消息队列
如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例.

如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.


如图，这个 Kafka 集群有两台 server 的，四个分区(p0-p3)和两个消费者组。消费组A有两个消费者，消费组B有四个消费者。

通常情况下，每个 topic 都会有一些消费组，一个消费组对应一个"逻辑订阅者"。一个消费组由许多消费者实例组成，便于扩展和容错。这就是发布和订阅的概念，只不过订阅者是一组消费者而不是单个的进程。

##
以下问题产生自文档4.3 efficency
1/consumer的批处理，是会读一批文件吗？
2/sendFile的具体实现
3/压缩也可以批处理吗？一批数据一起压缩

## Leader负载均衡
controller应该需要知道下面Patrtition的Leader都在哪里，以作负载均衡。
新加入的group leader可以手动设置吗？

度）消息 pull 到 log 当前位置的后面，从而使得数据能够得到最佳的处理而不会引入不必要的延迟。

## consumer阻塞？
简单的 pull-based 系统的不足之处在于：如果 broker 中没有数据，consumer 可能会在一个紧密的循环中结束轮询，实际上 busy-waiting 直到数据到来。为了避免 busy-waiting，我们在 pull 请求中加入参数，使得 consumer 在一个“long pull”中阻塞等待，直到数据到来（还可以选择等待给定字节长度的数据来确保传输长度）。

## 消息被消费后怎么处理？
也许不太明显，但要让 broker 和 consumer 就被消费的数据保持一致性也不是一个小问题。如果 broker 在每条消息被发送到网络的时候，立即将其标记为 consumed，那么一旦 consumer 无法处理该消息（可能由 consumer 崩溃或者请求超时或者其他原因导致），该消息就会丢失。 为了解决消息丢失的问题，许多消息系统增加了确认机制：即当消息被发送出去的时候，消息仅被标记为sent 而不是 consumed；然后 broker 会等待一个来自 consumer 的特定确认，再将消息标记为consumed。这个策略修复了消息丢失的问题，但也产生了新问题。 首先，如果 consumer 处理了消息但在发送确认之前出错了，那么该消息就会被消费两次。第二个是关于性能的，现在 broker 必须为每条消息保存多个状态（首先对其加锁，确保该消息只被发送一次，然后将其永久的标记为 consumed，以便将其移除）。 还有更棘手的问题要处理，比如如何处理已经发送但一直得不到确认的消息。

Kafka 使用完全不同的方式解决消息丢失问题。Kafka的 topic 被分割成了一组完全有序的 partition，其中每一个 partition 在任意给定的时间内只能被每个订阅了这个 topic 的 consumer 组中的一个 consumer 消费。这意味着 partition 中 每一个 consumer 的位置仅仅是一个数字，即下一条要消费的消息的offset。这使得被消费的消息的状态信息相当少，每个 partition 只需要一个数字。这个状态信息还可以作为周期性的 checkpoint。这以非常低的代价实现了和消息确认机制等同的效果。

这种方式还有一个附加的好处。consumer 可以回退到之前的 offset 来再次消费之前的数据，这个操作违反了队列的基本原则，但事实证明对大多数 consumer 来说这是一个必不可少的特性。 例如，如果 consumer 的代码有 bug，并且在 bug 被发现前已经有一部分数据被消费了， 那么 consumer 可以在 bug 修复后通过回退到之前的 offset 来再次消费这些数据。

## prodocuer exact-once
在 0.11.0.0 之前的版本中, 如果 producer 没有收到表明消息已经被提交的响应, 那么 producer 除了将消息重传之外别无选择。 这里提供的是 at-least-once 的消息交付语义，因为如果最初的请求事实上执行成功了，那么重传过程中该消息就会被再次写入到 log 当中。 从 0.11.0.0 版本开始，Kafka producer新增了幂等性的传递选项，该选项保证重传不会在 log 中产生重复条目。 为实现这个目的, broker 给每个 producer 都分配了一个 ID ，并且 producer 给每条被发送的消息分配了一个序列号来避免产生重复的消息。 同样也是从 0.11.0.0 版本开始, producer 新增了使用类似事务性的语义将消息发送到多个 topic partition 的功能： 也就是说，要么所有的消息都被成功的写入到了 log，要么一个都没写进去。这种语义的主要应用场景就是 Kafka topic 之间的 exactly-once 的数据传递(如下所述)。

## consumer exact-once

现在让我们从 consumer 的视角来描述语义。 所有的副本都有相同的 log 和相同的 offset。consumer 负责控制它在 log 中的位置。如果 consumer 永远不崩溃，那么它可以将这个位置信息只存储在内存中。但如果 consumer 发生了故障，我们希望这个 topic partition 被另一个进程接管， 那么新进程需要选择一个合适的位置开始进行处理。假设 consumer 要读取一些消息——它有几个处理消息和更新位置的选项。

两种场景，
1.流场景，可以在消费并写入新TOpic时，事务写入offset
2.其他场景，解决这一问题的经典方法是在 consumer offset 的存储和 consumer 的输出结果的存储之间引入 **two-phase commit**。但这可以用更简单的方法处理，而且通常的做法是让 consumer 将其 offset 存储在与其输出相同的位置。 这也是一种更好的方式，因为大多数 consumer 想写入的输出系统都不支持 two-phase commit。

## In sync

与大多数分布式系统一样，自动处理故障需要精确定义节点 “alive” 的概念。Kafka 判断节点是否存活有两种方式。

1. 节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连接。(这一点Multi-raft更好做，因为broker本身就保存了raftgroup的状态)
2. 如果节点是个 follower ，它必须能及时的同步 leader 的写操作，并且延时不能太久。

我们认为满足这两个条件的节点处于 “in sync” 状态，区别于 “alive” 和 “failed” 。 Leader会追踪所有 “in sync” 的节点。如果有节点挂掉了, 或是写超时, 或是心跳超时, leader 就会把它从同步副本列表中移除。 同步超时和写超时的时间由 replica.lag.time.max.ms 配置确定。


。Kafka 没有处理所谓的 “Byzantine” 故障，即一个节点出现了随意响应和恶意响应（可能由于 bug 或 非法操作导致）。



只有当消息被所有的副本节点加入到日志中时, 才算是提交, 只有提交的消息才会被 consumer 消费, 这样就不用担心一旦 leader 挂掉了消息会丢失。

另一方面， producer 也可以使用的 acks 选择是否等待消息被提交。 


## 存在的问题
计算和存储无法独立扩展，Broker是有状态的，扩容时数据需要迁移。

AutoMQ 则选择了一种完全不同的方法——复用 Kafka 协议层代码，以确保 100% 兼容性，同时重构底层存储，使 Broker 能够直接将数据写入 Object Storage，并通过引入 Write-Ahead Log 来避免带来额外延迟。

 AutoMQ Broker 首先将 Message 写入堆外内存缓存（Off-heap Memory Cache），进行批量聚合后再写入对象存储（如 S3）。为了保障在写入对象存储前出现故障时的数据持久性，AutoMQ 引入了可插拔的预写日志（WAL，Write-Ahead Log）机制。Broker 在返回消息写入成功的确认（Ack）前，必须先将消息写入 WAL 中，然后再异步写入对象存储。如果 Broker 故障，AutoMQ 会利用 WAL 中的数据进行恢复。

## kafka为什么不用raft存数据

大多数投票的缺点是，多数的节点挂掉让你不能选择 leader。要冗余单点故障需要三份数据，并且要冗余两个故障需要五份的数据。根据我们的经验，在一个系统中，仅仅靠冗余来避免单点故障是不够的，但是每写5次，对磁盘空间需求是5倍， 吞吐量下降到 1/5，这对于处理海量数据问题是不切实际的。这可能是为什么 quorum 算法更常用于共享集群配置（如 ZooKeeper ）， 而不适用于原始数据存储的原因，例如 HDFS 中 namenode 的高可用是建立在 基于投票的元数据 ，这种代价高昂的存储方式不适用数据本身。



## 待解决
Kafka的网络优化？
kafka增加broker的时候，是把整个log数据文件都发过去吗？需要等待完成？还是新Broker通过Pull的方式拉？
kakka的副本重分配逻辑

怎么理解下面这段话：另一个重要的设计区别是，Kafka 不要求崩溃的节点恢复所有的数据，在这种空间中的复制算法经常依赖于存在 “稳定存储”，在没有违反潜在的一致性的情况下，出现任何故障再恢复情况下都不会丢失。 这个假设有两个主要的问题。首先，我们在持久性数据系统的实际操作中观察到的最常见的问题是磁盘错误，并且它们通常不能保证数据的完整性。其次，即使磁盘错误不是问题，我们也不希望在每次写入时都要求使用 fsync 来保证一致性， 因为这会使性能降低两到三个数量级。我们的协议能确保备份节点重新加入ISR 之前，即使它挂时没有新的数据, 它也必须完整再一次同步数据。

consumer group的offset在哪里记录？

kafka的broker好像是注册到zookper的，需要用etcd换吗？


## 用multi-raft 好处
multi-raft和元数据共享链接，减轻网络和维护ISR压力
数据强一致，可通过removeGroup对每个Partition灵活调整副本数（如果partition是按一定规则分配的，那就更有意义）
kafka冗余度更高，能容忍n个节点失联，（raft可以动态调整多数派的值吗？）但可能丢失数据（如有有ISR机制，真的会丢吗），同时ISR机制同时也会被最慢的节点拖垮

## 一致性
但是，实际在运行的系统需要去考虑假设一旦所有的备份都挂了，怎么去保证数据不会丢失，这里有两种实现的方法

等待一个 ISR 的副本重新恢复正常服务，并选择这个副本作为领 leader （它有极大可能拥有全部数据）。
选择第一个重新恢复正常服务的副本（不一定是 ISR 中的）作为leader。
这是可用性和一致性之间的简单妥协，如果我只等待 ISR 的备份节点，那么只要 ISR 备份节点都挂了，我们的服务将一直会不可用，如果它们的数据损坏了或者丢失了，那就会是长久的宕机。另一方面，如果不是 ISR 中的节点恢复服务并且我们允许它成为 leader ， 那么它的数据就是可信的来源，即使它不能保证记录了每一个已经提交的消息。 kafka 默认选择第二种策略，当所有的 ISR 副本都挂掉时，会选择一个可能不同步的备份作为 leader ，可以配置属性 unclean.leader.election.enable 禁用此策略，那么就会使用第 一种策略即停机时间优于不同步。

# CV
## Raft相关优化
1.在Raft前并发，放到Raft log里的是KV结果。不放在Raft后并发是考虑到正确性以及并发发生冲突时处理的简单性。
具体做法是维护了多个队列，每个队列由一个go处理，队列内串行执行以减小冲突，不同队列中的任务可以并发执行，乐观锁模式，记录开始及结束时间戳，提交时检查是否重叠选择是否重新执行
2.不同客户端的先后顺序无法完全保证，业界也没有解决方案(?)，除非实现复杂的客户端间协议。
3.ReadIndex消耗其实很大，由于业务的特点（3个节点Raft，不会出现false leader(follwer和follwer认为的Leader中至少有一个知道最新的状态），因此我们主动发送，10s请求一次。同时，根据文件系统本身的特性，如果一个文件一段时间内没有修改，且可以只读本地KV，那么可以直接执行LocalRead（同样不保证多客户端的）


